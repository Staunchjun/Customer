{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import pandas as pd\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "def delUnname0(df):\n",
    "    df = df.drop('Unnamed: 0',axis=1)\n",
    "    return df\n",
    "def xgb_eval_custom_r(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    up = np.sum(y_pred - y_true)\n",
    "    down = np.sum(y_pred + y_true)\n",
    "    whole = np.abs(up/down)\n",
    "    Loss = whole/len(y_true)\n",
    "    #here only predict one shop\n",
    "    return 'Loss', Loss\n",
    "#preprocessing normailze?No.Do a one hot encoding~~Because the data is so small~~\n",
    "def TimeFeat(df):\n",
    "    columns = df.columns\n",
    "    df['daysinmonth'] = pd.Index(df[columns[1]]).daysinmonth\n",
    "    df['weekofyear'] = pd.Index(df[columns[1]]).weekofyear\n",
    "    df['dayofweek'] = pd.Index(df[columns[1]]).dayofweek\n",
    "    dummies_daysinmonth = pd.get_dummies(df['daysinmonth'],prefix='daysinmonth')\n",
    "    dummies_weekofyear = pd.get_dummies(df['weekofyear'],prefix='weekofyear')\n",
    "    dummies_dayofweek = pd.get_dummies(df['dayofweek'],prefix='dayofweek')\n",
    "    df_dummies = df.join([dummies_daysinmonth,dummies_weekofyear,dummies_dayofweek],how='outer')\n",
    "    df_dummies = df_dummies.drop(['dayofweek','daysinmonth','weekofyear'],axis=1)\n",
    "    return df_dummies\n",
    "def LoadData():\n",
    "    pd.set_option('display.max_columns', 120)\n",
    "    Customer_Flow = pd.read_csv(\"Customer_Flow.csv\")\n",
    "    Customer_Flow = delUnname0(Customer_Flow)\n",
    "    Customer_Flow['data'] = pd.to_datetime(Customer_Flow['data'])\n",
    "    return  Customer_Flow\n",
    "def GenerateAllData(Customer_Flow):\n",
    "    p_startt = pd.to_datetime(\"2016-11-01\")\n",
    "    p_endt = pd.to_datetime(\"2016-11-14\")\n",
    "    p_time = pd.date_range(start=p_startt, end=p_endt, freq='D')\n",
    "    p_shop_id = range(1, 2001, 1)\n",
    "    shopid = []\n",
    "    p_t = []\n",
    "    for shop_id in p_shop_id:\n",
    "        for t in p_time:\n",
    "            shopid.append(shop_id)\n",
    "            p_t.append(t)\n",
    "    predict = {'data': p_t, 'shop_id': shopid}\n",
    "    predict = pd.DataFrame(predict)\n",
    "    all_data = Customer_Flow.merge(predict, how='outer')\n",
    "    return all_data\n",
    "def GenerateTimeF(eachShop):\n",
    "    eachShop.reset_index(inplace=True)\n",
    "    eachShop.drop(['index'],inplace=True,axis=1)\n",
    "    Yesterday_col = {}\n",
    "    TwoDayAgo_col = {}\n",
    "    TwoDayAgo_col = pd.DataFrame(TwoDayAgo_col)\n",
    "    Yesterday_col = pd.DataFrame(Yesterday_col)\n",
    "    Yesterday_col['yesterday'] = eachShop['Num']\n",
    "    TwoDayAgo_col['TwoDayAgo_col'] = eachShop['Num']\n",
    "\n",
    "    eachShop = eachShop.drop(0).drop(1)\n",
    "    eachShop.reset_index(inplace=True)\n",
    "    eachShop.drop(['index'],inplace=True,axis=1)\n",
    "\n",
    "    Yesterday_col = Yesterday_col.drop(0)\n",
    "    Yesterday_col.reset_index(inplace=True)\n",
    "    Yesterday_col.drop(['index'],inplace=True,axis=1)\n",
    "\n",
    "    TwoDayAgo_col.reset_index(inplace=True)\n",
    "    TwoDayAgo_col.drop(['index'],inplace=True,axis=1)\n",
    "\n",
    "    newdata = TwoDayAgo_col.join(Yesterday_col)\n",
    "    newdata = newdata.join(eachShop)\n",
    "    newdata.drop(['shop_id'],axis=1,inplace=True)\n",
    "    newdata['difference_two_day'] = newdata['yesterday'] - newdata['TwoDayAgo_col']\n",
    "    newdata.drop(len(newdata)-1,inplace=True)\n",
    "    newdata.drop(len(newdata)-1,inplace=True)\n",
    "    basePredict = newdata.ix[len(newdata)-14:len(newdata)-1]\n",
    "    for x in range(0,14):\n",
    "        newdata.drop(len(newdata)-1,inplace=True)\n",
    "    return basePredict,newdata\n",
    "def SplitData(PredictData,newdata):\n",
    "    PredictData = PredictData.reset_index()\n",
    "    PredictData.drop(['index'],axis=1,inplace=True)\n",
    "    train_data = newdata.ix[0:int(0.9*len(newdata))]\n",
    "    test_data = newdata.ix[int(0.9*len(newdata)):len(newdata)-1]\n",
    "    return PredictData,train_data,test_data\n",
    "def FittingModel(train_data,test_data,ToDrop):\n",
    "    train_xgb = xgb.DMatrix(data=train_data.drop(ToDrop,axis=1)\n",
    "                            ,label=train_data['Num'])\n",
    "    valid_xgb = xgb.DMatrix(data=test_data.drop(ToDrop,axis=1)\n",
    "                            ,label=test_data['Num'])\n",
    "\n",
    "    params = {\n",
    "      'objective': 'reg:linear'\n",
    "      ,'eta': 0.1\n",
    "      ,'max_depth': 6\n",
    "      , 'subsample': 0.4\n",
    "      , 'colsample_bytree': 0.9\n",
    "      ,'min_child_weight': 12\n",
    "      ,'gamma': 0.07\n",
    "      , 'seed': 10\n",
    "    ,'reg_alpha': 0.06\n",
    "    }\n",
    "    evallist = [(train_xgb, 'train'), (valid_xgb, 'valid')]\n",
    "    model = xgb.train(params.items()\n",
    "                      , dtrain=train_xgb\n",
    "                      , num_boost_round=10000\n",
    "                      , evals=evallist\n",
    "                      , early_stopping_rounds=20\n",
    "                      , maximize=False\n",
    "                      , verbose_eval=10\n",
    "                      , feval=xgb_eval_custom_r\n",
    "                  )\n",
    "    print \"now it is:\",shop_id\n",
    "    print ('get info from model')\n",
    "    print (model.best_score, model.best_iteration,model.best_ntree_limit)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_result = []\n",
    "Customer_Flow = LoadData()\n",
    "all_data = GenerateAllData(Customer_Flow)\n",
    "for shop_id, eachShop in all_data.groupby(['shop_id']):\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    eachShop = TimeFeat(eachShop)\n",
    "    PredictData, newdata = GenerateTimeF(eachShop)\n",
    "    PredictData,train_data, test_data = SplitData(PredictData,newdata)\n",
    "    ToDrop = ['Num', 'data']\n",
    "    print \"fitting data on model.....\"\n",
    "    model =  FittingModel(train_data,test_data,ToDrop)\n",
    "    result = []\n",
    "    dataLen = len(PredictData)\n",
    "    print \"predicting....\"\n",
    "    for x in range(0, dataLen):\n",
    "        first = PredictData.ix[x:x]\n",
    "        if x == 1:\n",
    "            first['yesterday'] = firstNum\n",
    "            Next_TwoDayAgo_col = firstNum\n",
    "        if x >= 2:\n",
    "            first['yesterday'] = firstNum\n",
    "            first['TwoDayAgo_col'] = Next_TwoDayAgo_col\n",
    "            Next_TwoDayAgo_col = firstNum\n",
    "        predict_xgb = xgb.DMatrix(data=first.drop(ToDrop, axis=1))\n",
    "        firstNum = model.predict(predict_xgb, ntree_limit=model.best_ntree_limit)\n",
    "        result.append(firstNum)\n",
    "    result = pd.DataFrame(result)\n",
    "    result = result.rename(columns={0:'p'})\n",
    "    each_line = []\n",
    "    each_line.append(shop_id)\n",
    "    for Num in result.p:\n",
    "        each_line.append(Num)\n",
    "    p_result.append(each_line)\n",
    "    print \" the task  of \",shop_id,\"is finished!!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}